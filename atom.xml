<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://yangcheng-swufe.github.io/</id>
    <title>NLP-Swufe</title>
    <updated>2019-06-23T14:45:33.985Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://yangcheng-swufe.github.io/"/>
    <link rel="self" href="https://yangcheng-swufe.github.io//atom.xml"/>
    <subtitle>A new begin for nlp</subtitle>
    <logo>https://yangcheng-swufe.github.io//images/avatar.png</logo>
    <icon>https://yangcheng-swufe.github.io//favicon.ico</icon>
    <rights>All rights reserved 2019, NLP-Swufe</rights>
    <entry>
        <title type="html"><![CDATA[The Illustrated Transformer]]></title>
        <id>https://yangcheng-swufe.github.io//post/the-illustrated-transformer</id>
        <link href="https://yangcheng-swufe.github.io//post/the-illustrated-transformer">
        </link>
        <updated>2019-06-22T16:33:23.000Z</updated>
        <content type="html"><![CDATA[<p>In this first post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.</p>
<p>The Transformer was proposed in the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>. A TensorFlow implementation of it is available as a part of the
<a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a> package. Harvard’s NLP group created a <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">guide annotating the paper with PyTorch implementation</a>. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.</p>
<h2 id="a-high-level-look">A High-Level Look</h2>
<p>Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.
<img src="https://yangcheng-swufe.github.io//images/transformer.png" alt=""></p>
]]></content>
    </entry>
</feed>